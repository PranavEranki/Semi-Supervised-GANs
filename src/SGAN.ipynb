{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# os.makedirs('images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set Defaults\n",
    "'''\n",
    "num_epochs = 200 \n",
    "batch_size = 64\n",
    "num_classes = 10 # number of classes for dataset\n",
    "lr = 0.0002 \n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "n_cpu = 8 # number of cpu threads to use during batch generation\n",
    "latent_dim = 100 # dimensionality of the latent space\n",
    "img_size = 32 # size of each image dimension\n",
    "channels = 1 # number of output image channels\n",
    "sample_interval = 400 # interval between image sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set cuda\n",
    "if torch.cuda.is_available():\n",
    "    cuda = True \n",
    "else:\n",
    "    cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_size // 4 # Initial size before upsampling\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128*self.init_size**2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.linear(noise)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channels, out_channels=16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(num_features=32, eps=0.8)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(num_features=64, eps=0.8)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.BatchNorm2d(num_features=128, eps=0.8)\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2**4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=128*ds_size**2, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.aux_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=128*ds_size**2, out_features=num_classes+1),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv1(img)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        \n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DataLoader\n",
    "DATA_FOLDER = './torch_data/MNIST'\n",
    "\n",
    "def mnist_data():\n",
    "    compose = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "data = mnist_data()\n",
    "dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining ground-truth for real and fake images\n",
    "\n",
    "def real_data_groundtruth(size):\n",
    "    '''\n",
    "    Variable containing ones, with shape = size, 1\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1), requires_grad=False)\n",
    "    if torch.cuda.is_available(): \n",
    "        return data.cuda()\n",
    "    return data\n",
    "\n",
    "def fake_data_groundtruth(size):\n",
    "    '''\n",
    "    Variable containing zeros, with shape = size, 1\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1), requires_grad=False)\n",
    "    if torch.cuda.is_available(): \n",
    "        return data.cuda()\n",
    "    return data\n",
    "\n",
    "def fake_aux_groundtruth(size):\n",
    "    '''\n",
    "    Variable containing num_classes+1, with shape = size\n",
    "    '''\n",
    "    data = Variable(LongTensor(size).fill_(num_classes), requires_grad=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    n = Variable(torch.randn(size, latent_dim))\n",
    "    if torch.cuda.is_available(): \n",
    "        return n.cuda() \n",
    "    else:\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer_D, real_imgs, fake_imgs, labels):\n",
    "    optimizer_D.zero_grad()\n",
    "    \n",
    "    # Loss for real images\n",
    "    real_pred, real_aux = discriminator(real_imgs)\n",
    "    d_real_loss =  (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2.0\n",
    "\n",
    "    # Loss for fake images\n",
    "    fake_pred, fake_aux = discriminator(fake_imgs)\n",
    "    d_fake_loss =  (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, fake_aux_gt)) / 2.0\n",
    "\n",
    "    # Total discriminator loss\n",
    "    d_loss = (d_real_loss + d_fake_loss) / 2.0\n",
    "    \n",
    "    # Calculate discriminator accuracy\n",
    "    pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "    gt = np.concatenate([labels.data.cpu().numpy(), fake_aux_gt.data.cpu().numpy()], axis=0)\n",
    "    d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "        \n",
    "    return d_loss, d_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(optimizer_G, gen_imgs):\n",
    "    optimizer_G.zero_grad()\n",
    "    \n",
    "    # Loss measures generator's ability to fool the discriminator\n",
    "    validity, _ = discriminator(gen_imgs)\n",
    "    g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    \n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start Training\n",
    "'''\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        batch_size_ = imgs.shape[0]\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = real_data_groundtruth(batch_size_)\n",
    "        fake = fake_data_groundtruth(batch_size_)\n",
    "        fake_aux_gt = fake_aux_groundtruth(batch_size_)\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "        \n",
    "        ###############################################\n",
    "        #              Train Generator                #\n",
    "        ###############################################\n",
    "        \n",
    "        gen_imgs = generator(noise(batch_size_))\n",
    "        g_loss = train_generator(optimizer_G, gen_imgs)\n",
    "        \n",
    "        ###############################################\n",
    "        #              Train Discriminator            #\n",
    "        ###############################################   \n",
    "\n",
    "        fake_imgs = generator(noise(batch_size_)).detach()\n",
    "        d_loss, d_acc = train_discriminator(optimizer_D, real_imgs, fake_imgs, labels)\n",
    "        \n",
    "        # Display Progress\n",
    "        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\" % (epoch, num_epochs, i, len(dataloader),\n",
    "                                                            d_loss.item(), 100 * d_acc,\n",
    "                                                            g_loss.item()))\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
